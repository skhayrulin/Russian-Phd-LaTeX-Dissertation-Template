\chapter{Форматы данных для организации взаимодействия различных программно-аппаратных архитектур}\label{ch:ch3}

\section{Модель размещения данных.}\label{sec:ch3/sect1}

Для описания информации об эволюции численной модели была предложена базовая структура, хранящие данные о состоянии частицы как физические величины: позиция, скорость, ускорение, плотность, масса, вязкость, давление в каждый момент времени так и характеристические такие как идентификатор (\(particleID\)), идентификатор пространственной ячейки (\(cellID\)), тип частицы рисунок ~\ref{fig:p_struct}.
\begin{figure}[ht]
  \centerfloat{
    \includegraphics[scale=0.30]{p_struct}
  }
  \caption{Структура частицы.}\label{fig:p_struct}
\end{figure}

Таким образом модель описывается массивом частиц. При этом в зависимости от требования порядку точности вычислений можно варьировать обобщенный тип T между двумя интегральными типами данных \(float\) или \(double\). Каждая партиция определяется следующим образом пусть \(N\) – количество частиц \((p_0,...,p_{N-1})\) – упорядоченное по \(cellID\) множество начальных данных о частицах. Подразумевается, что каждый элемент \(p_i\) хранит пространственные координаты, координаты вектора скорости и \(cellID\) \(i\)-й частицы как показано на рисунке ~\ref{fig:p_struct}. Определим пространственные параметры модели
\((x_{min}, y_{min}, z_{min})\), \((x_{max}, y_{max}, z_{max})\) – точки, определяющие границы области моделирования (вершины параллелепипеда, лежащие на его диагонали).
\(gridCellX\), \(gridCellY\), \(gridCellZ\)- количество пространственных ячеек по соответствующим измерениям трехмерного пространства. Эти значения получаются из целочисленного деления длины ребра ограничивающего объема на длину ребра пространсвенной ячейки например
\[
  gridCellX = \left \lfloor \frac{\left |x_{max} - x_{min}  \right |}{2h} \right \rfloor
\]
\[
  gridCellY = \left \lfloor \frac{\left |y_{max} - y_{min}  \right |}{2h} \right \rfloor
\]
\[
  gridCellZ = \left \lfloor \frac{\left |z_{max} - z_{min}  \right |}{2h} \right \rfloor
\]
\(h\) – радиус сглаживания,
\(M\)– количество доступных устройств.

Для сравнительной оценки производительности устройства вводится эвристическая функция, которая рассчитывает коэффициент производительности на основе возможного количества потоков, которые можно одновременно запустить на конкретном устройстве:
\[
  \epsilon(d_i)=D \cdot WG
\]
где \(d_i\) – устройство, \(D\)- количество доступных стриминговых мультипроцессоров (streaming multiprocessor - SM), для CPU – это число равно количеству ядер, \(WG\) – размерность рабочей группы для конкретного устройства. Например GPU Radeon R 290X обладает 44 стриминговых ядер, \(WG=256\).

Для достижения синхронности времени работы, необходимо, чтобы перед каждой итерацией данные были распределены между устройствами в количестве пропорциональном производительности устройств. Оптимальное количество частиц для обработки \(i\)-ым устройством определяется по следующей формуле:
\[
  N_{i}^{'}=\left [ N \cdot \frac{\epsilon(d_i)}{\sum_{j}\epsilon(d_j)} \right ]
\]

Сформулируем еще два условия:
\noindent
\begin{enumerate}
  \item Все частицы, лежащие одной ячейке, должны обрабатываться одним устройством. Это условие можно сформулировать следующим образом:
        \((C)\) \(\forall p_i, p_j\) \textit{если \(cellID\) частицы \(p_i=cellID\) частицы \(p_i\), то \(p_i, p_j\) обрабатываются одним устройством.}
  \item Количество пространственных ячеек обрабатываемых одним устройством должно быть кратным \(gridCellY\).
\end{enumerate}

Следствием наложения этого условий является то, что итоговое количество частиц \(N_i \) для обработки \(i\)-м устройством  может отличаться от \(N_{i}^{'}\) (в зависимости от количества частиц в ячейке) т.е.:
\[
  N_i = N_{i}^{'}+\Delta N_i, i=0,..., M-1
\]
При этом:
\[
  \sum_{j} N_j = \sum_{j}N_{j}^{'}=N
\]

Введем определение структуры партиции \(partition_i\) – структура для хранения индексов в общем массиве данных первой \(partition_{i}.start\) и следующей после последней частицы \(partition_{i}.end\), обрабатываемой \(i\)-м sudo snap install --classic goустройством с соблюдением условия
\((C)\), которое достигается за счет упорядоченности множества частиц по номеру ячейки.
\[
  partition_{0}.start = 0
\]
\[
  partition_{0}.end = partition_{0}.start + \epsilon (d_0) \cdot N + OFFSET_0
\]
\[
  ...
\]
\[
  partition_{i}.start = partition_{i-1}.end + 1
\]
\[
  partition_{i}.end = partition_{i}.start + \epsilon (d_i) \cdot N + OFFSET_i, i=2,..., M - 1
\]
\(OFFSET_i\) – определяет количество частиц, которые находятся в добавочных ячейках (см. условие 2).
Заданные выше партиции определяют подмножества частиц, обрабатываемые соответствующими устройствами. Таким образом достигается распределение данных между устройствами так, что группы данных не пересекаются друг с другом. Как было уже сказано выше для корректности расчетов для частиц, которые находиться на границах партиций необходимо также учитывать частицы находящиеся в граничных ячейках соседних партиций.Устройство с номером  получает на обработку упорядоченный по  набор частиц. \fixme{К этому набору применяется параллельный метод PCI SPH. // тут нужно дописать}

\section{Модель вычислений.}\label{sec:ch3/sect2}

Модель вычислений определяет абстрактное представление того, как потоки инструкций выполняются в гетерогенной системе. Управляющая часть программы описывает, структуру, контролирующую ход вычислений и синхронизирует вычислительные узлы. Узел - отдельное независимое устройство GPU/CPU, обладающее изолированной памятью. В зависимости от количества узлов создается соответствующее количество параллельных потоков, выполняющих код отдельно, но в одном адресном пространстве. Каждый поток резервирует узел и контролирует вычисления на нем. Вычисления на узле могут проходить параллельно. Модель вычислений представлена на рисунке ~\ref{fig:calc1}.
\begin{figure}[ht]
  \centerfloat{
    \includegraphics[scale=0.30]{calc1}
  }
  \caption{Модель вычислений.}\label{fig:calc1}
\end{figure}

Как видно из схемы, на этапе синхронизации все потоки  приостанавливаются, и управляющий поток синхронизирует данные между вычислителями, после чего вновь активирует их для дальнейшей работы. Синхронизация данных включает в себя процесс упорядочивания/сортировки массива частиц по соответствующему значению номера пространственной ячейки, актуализация массивов частиц в оперативной памяти устройств.

Как показывают графики зависимости производительности от количества частиц для различных конфигураций время вычисления одной итерации моделирования сильно коррелирует с процессом синхронизации и, в значительной мере, сортировке ~\ref{fig:result}. Для  конфигураций с количеством частиц, начиная от нескольких миллионов, сортировка может занимать до \(\sim \)50\% времени расчета итерации. В программной библиотеке sibernetic \cite{Palyanov2016} процесс упорядочивания может работать в двух режимах: параллельном и последовательном.

При последовательном режиме используется быстрая сортировка quick sort \cite{Hoare1962}, реализованная в стандартной библиотеке шаблонов stl \cite{Stepanov1995} для языка программирования C++ \cite{Stroustrup2013}. При этом асимптотическая сложность данного алгоритма равна \( O(N \cdot log(N)) \). Это позволяет довольно эффективно обрабатывать небольшие массивы до миллиона элементов и как показывают графики быстрее чем параллельная реализация.

В параллельном режиме реализована модификация алгоритма цифровой сортировки предложенная в работах \cite{Knuth1998, Marcho1991}. Это позволило значительно ускорить процесс синхронизации как показано на рисунке ~\ref{fig:result}. При этом асимптотическая сложность алгоритма равна \( 2^{r-1} \cdot O(\frac{N}{M}) \), где \( r \) - количество бит на один разряд для целого числа \( int32 \) это значение равно 4, \( N \) - количество элементов в массиве, \( M \) - количество потоков на вычислительном устройстве. Для того чтобы минимизировать количество перестановок в памяти комплексных структур, описывающих частицу ~\ref{fig:p_struct}, строится актуальная перестановка специального массива  индексов частиц, задающего взаимно-однозначное соответствие между текущим множеством и упорядоченным. Таким образом необходимость выделения избыточной памяти ограничивается лишь одним целым числом.
Процесс переупорядочивания также выполняется параллельно. На рисунке ~\ref{fig:sort1} показана.
\begin{figure}[ht]
  \centerfloat{
    \includegraphics[scale=0.30]{sort1}
  }
  \caption{Схема работы процесса сортировки}\label{fig:sort1}
\end{figure}

\section{Параллельная реализация в системе программирования OpenCL.}\label{sec:ch3/sect3}

Для работы с параллельными вычислениями была выбрана платформа OpenCL, предназначенная для создания приложений, связанных с вычислениями на гетерогенных вычислительных системах, стандарт обеспечивает параллелизм на уровне инструкций и на уровне  данных и является реализацией техники GPGPU  \cite{Munshi2011, Stone2010}. Основными  преимуществами OpenCL являются открытость стандарта и поддержка большинством основных производителей как комплектующих, так и программного обеспечения, например Intel, AMD, NVIDIA, Apple (более подробный список на сайте http://www.khronos.org/opencl/). Таким образом это позволяет писать код, который можно запускать на различных устройствах GPU, CPU, FPGA. Код написанный на языке OpenCL интерпретируется соответствующим  компилятором, например, для GPU от компании NVIDIA компилятор OpenCL встроен в драйвер библиотеки CUDA \cite{Cook2012}.

Спецификация OpenCL интерпретирует любую платформу как хост-систему (host) и связанной с одним или более устройством, поддерживающим OpenCL. Каждое устройство состоит из одного или более вычислительных модулей - compute units,  которые  могут  включать  в  себя  несколько обрабатывающих элементов - processing elements. Вычисления  происходят  в обрабатывающих элементах устройства. В данном контексте хост-система выступает в качестве контроллера, управляющего потоком  данных и конвейером вычислений. Обрабатывающие элементы модуля могут выполнять поток инструкций как единицы SIMD(single instruction, multiple data)\cite{Flynn1972} или как единицы SPMD(single program, multiple data streams)\cite{Darema2011}.

Стоит помнить о том, что передача данных по внутренеей шине модуля зачастую быстрее чем обмен данными между хост-программой и модулем. Таким образом данный фактор может стать лимитирующим при неэффективной организации загрузки модуля. Вычисления на устройстве описываются специальными функциями ядрами (kernel) написанными на расширенном диалекте языка С стандарта С99 \cite{Kernighan1988, ISO:C99}. Перед запуском ядра определяется пространство индексов размерность этого пространства в сответствие с стандартом может быть равна 1, 2 или 3. Для каждой точки данного пространства выполняется экземпляр ядра, который называется элементом работы. Соответствующая точка в пространстве индексов задает его глобальный идентификатор. Таким образом при обработки массива данных можно реализовать параллелизм по данным, который является основным сценарием использования OpenCL. Элементы работы объединяются в группы работ, которые обеспечивают более крупнозернистую  декомпозицию пространства индексов. Группам работ также назначаются уникальные идентификаторы, размерность которых совпадает с размерностью пространства индексов. В пределах  одной группы работ каждый элемент получает уникальный локальный идентификатор. Таким образом, элемент работы определяется двумя способами: своим глобальным идентификатором или комбинацией локального идентификатора и идентификатора группы работ ~\ref{fig:ndrange}.
\begin{figure}[ht]
  \centerfloat{
    \includegraphics[scale=0.80]{ndrange}
  }
  \caption{Пример индексного пространства NDRange, показывающего рабочие элементы, их глобальные идентификаторы и их сопоставление с парой рабочих групп и локальных идентификаторов.}\label{fig:ndrange}
\end{figure}
Так же OpenCL предатставляет возможность огранизации параллелизма по инструкциям, засчет организации асинхронных конвееров вычислений на основе событийной модели.

Память в стандарте разделяется на несколько категорий в зависимости от типа, размера и скорости доступа:

\noindent Вложенные списки:
\begin{itemize}
  \item Глобальная память - оперативная память устройства данная память доступна для чтения и записи всеми элементам работы.
  \item Константная память - выделенная область  глобальной  памяти, которая  доступна только для чтения всем элементам работы и остается постоянной во время исполнения ядра.
  \item Локальная память - пространство памяти доступно для чтенияи записи элементам, принадлежащим одной группе работ, и может эффективно использоваться для доступа к общим переменным. Обычно в качестве такой памяти выступает кеш процессора.
  \item Частная память - несколько регистров памяти, доступной только для чтения и записи лишь одному элементу работы. Переменные, объявленные в частной памяти одного элемента работы, не видимы для другого элемента работы.
\end{itemize}


\section{Результаты тестирования и оценки.}\label{sec:ch3/sect4}
При   разработке   метода,   его   тестировании   и   верификациивозникает необходимость проведения большого числа расчётов. Крометого для проверки алгоритма распределения нагрузки вычислений исинхронизации данных необходима проверка его на машинах, имеющихнесколько вычислительных узлов (GPU). В основе предлагаемого намиалгоритма   лежит   идея   распределения   данных   по   доменам,   присоблюдении условия, что все данные в пределах каждого домена могутобрабатываться независимо. При этом расчёты не производятся длячастиц из смежных доменов. В силу специфики задачи данные обизменениях   позиций   частиц   своевременно   синхронизируются   междуустройствами.   Предполагается,   что   параллельные   вычисления   длякаждого домена будут производиться на различных GPU одновременно,кроме   того   каждый   контроллер   каждого   решателя,   запускается   вотдельном   потоке.   Таким   образом   одна   итерация   симуляциипредполагает несколько стадий:
\noindent Вложенные списки:
\begin{itemize}
  \item Формирование списка соседей для каждой частицы.
  \item Расчёт изменения физических величин и сглаживание флуктуацийплотности (PCI SPH).
  \item Численное интегрирование (Leapfrog, Semi-implicit Euler)
  \item Синхронизация данных
        \begin{itemize}
          \item Сортировка, 1 поток qsort, параллельная — модификацияпоразрядной сортировки
          \item Обновление данных на всех устройствах
        \end{itemize}
\end{itemize}
Было проведён ряд тестов для различных моделей описывающих симуляцию обрушения массива жидкости, которое возникает приразрушение дабы (damb break). При этом сами модели отличались размером и, соответвенно, количеством частиц - 96368, 844203, 1183724, 3547755, 6904779, 9772237, 14204179, 20078971. Тесты прогонялись для различных конфигураций вычислительного кластера, то есть варьировалось число сопроцессоров в системе от 1 до 8 GPU. Тесты проводились на вычислитеном крастере Новосибирского Государственного Университета на узле, имеющем два 20-ядерных процессора Xeon Gold 6248, 384 ГБ оперативной памяти и 8 GPU NVIDIA Tesla V100 SXM2 32GB. При каждом запуске логировалось время выполнения итерации как сумма:
\[
  T_{total}^{i} = T_{ns}^{i} + T_{phys}^{i} + T_{integration}^{i} + T_{sync}^{i}
\]

Где \(T_{total}^{i}\) - общее время выполнения итерации \(i\), \(T_{ns}^{i}\) - поиск соседей, \(T_{phys}^{i}\) - обновление физических   параметров, \(T_{integration}^{i}\) - численное интегрирование, \(T_{sync}^{i}\) - синхронизация. На этапе синхронизации происходит сортировка массива частиц и распределение новыхдоменных конфигураций по устройствам. Сортировка может работать вдвух режимах  последовательном и параллельном. Для каждого теста было произведено фиксированное количество итераций и затем вычисляется среднее время выполнения одной итерации как:
\[
  t_{average} = \frac{\sum_{i=1}^{N}T_{i}^{total}}{N}
\]
Полученные результаты позволяют судить о том, что нам удалось достичь значительного ускорения расчётов для дискретных моделей описанных с помощью метода класса PCI SPH \ref{fig:result}.
\begin{figure}[ht]
  \centerfloat{
    \includegraphics[scale=0.20]{result}
  }
  \caption{Символом || обозначен тесты, в которых для сортировки используется параллельная реализация.}\label{fig:result}
\end{figure}